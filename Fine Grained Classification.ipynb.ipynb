{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Fine-Grained Classification"
      ],
      "metadata": {
        "id": "imki18m10_oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions on How to run the model\n",
        "\n",
        "To execute the model, ensure you follow these steps:\n",
        "\n",
        "1. Mount a Google drive to a specified path.\n",
        "2. **Import all required libraries** and then later load the dataset using `section 2`.\n",
        "3. Run the functions within Image Preprocessing and Feature Engineering.\n",
        "4. We can **skip the Model building process** as I've already trained the model.\n",
        "5. Run the `evaluation function` to use it later during testing the model.\n",
        "6. No need to retrain the model by executing `Section 7`.\n",
        "7. If `test.csv` file contains filenames and images are stored in the designated directory, then you can process with the `Model Testing` section."
      ],
      "metadata": {
        "id": "woZ2x0I7ccYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "yIRCiEcD07LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Mounting a drive\n",
        "\n",
        "We'll first mount the drive to the directory '/content/drive'. Afterward, we'll switch the working directory to the folder containing this file."
      ],
      "metadata": {
        "id": "lXdkaUjnjOFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/Deep Learning/Coursework/Task 2\")"
      ],
      "metadata": {
        "id": "8Mr8blZOohyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6c03d2-d1e5-4249-c78f-6d66e7cef1a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Required Libraries"
      ],
      "metadata": {
        "id": "JSIHlel6m6hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.data.experimental import AUTOTUNE\n",
        "from tensorflow.io import read_file\n",
        "from tensorflow.image import decode_jpeg, resize, random_flip_left_right, crop_to_bounding_box\n",
        "\n",
        "\n",
        "# For AUTOTUNE\n",
        "AUTO = AUTOTUNE\n",
        "\n",
        "SEED_VALUE = 125\n",
        "np.random.seed(SEED_VALUE)\n",
        "tf.compat.v1.set_random_seed(SEED_VALUE)\n",
        "\n",
        "ROOT_FOLDER = \"/content/drive/MyDrive/Deep Learning/Coursework/Task 2\""
      ],
      "metadata": {
        "id": "gAR_5lXfoCxn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Data\n",
        "\n",
        "1. The images are store inside directory `./classification_aircraft/fgvc-aircraft-2013b/data/images`.\n",
        "2. I've got three different csv files for training, validation, testing. Later in the code, I stored full path of the images (training, validation, testing) in `train_paths`, `val_paths`, `test_paths`.\n",
        "3. The labels are undergoing conversion to categorical format.\n",
        "\n"
      ],
      "metadata": {
        "id": "BmfV4rOJux3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(f'{ROOT_FOLDER}/classification_aircraft/train.csv')\n",
        "test_data = pd.read_csv(f'{ROOT_FOLDER}/classification_aircraft/test.csv')\n",
        "val_data = pd.read_csv(f'{ROOT_FOLDER}/classification_aircraft/val.csv')\n",
        "\n",
        "IMG_PATH = f'{ROOT_FOLDER}/classification_aircraft/fgvc-aircraft-2013b/data/images'\n",
        "train_paths = train_data.filename.apply(lambda x: os.path.join(IMG_PATH, x))\n",
        "train_labels = to_categorical(train_data.Labels)\n",
        "\n",
        "val_paths = val_data.filename.apply(lambda x: os.path.join(IMG_PATH, x))\n",
        "val_labels = to_categorical(val_data.Labels)\n",
        "\n",
        "test_paths = test_data.filename.apply(lambda x: os.path.join(IMG_PATH, x))\n",
        "test_labels = to_categorical(test_data.Labels)\n"
      ],
      "metadata": {
        "id": "PM0TgD16oPJJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Image Preprocessing\n",
        "\n",
        "1. In the below function, `decode_jpeg` is from TensorFlow's image module used to decode JPEG-encoded images into a format that TensorFlow can manipulate and further process or analyzed using TensorFlow operations\n",
        "2. Later in the function, the image resized to given image_size (i.e. (521, 521))."
      ],
      "metadata": {
        "id": "9kicpsapuz1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_decoding(filename, label=None, image_size=(512, 512)):\n",
        "    bits = read_file(filename)\n",
        "    img = decode_jpeg(bits, channels=3)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    img = resize(img, image_size)\n",
        "\n",
        "    if label is None:\n",
        "        return img\n",
        "    else:\n",
        "        return img, label\n"
      ],
      "metadata": {
        "id": "u4iPdqRhoSZQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Feature Engineering\n",
        "\n",
        "1. The function `random_flip_left_right` randomly flips images from left to right. This operation is specifically used for data augmentation.\n",
        "2. `crop_to_bounding_box` function crops the image in specified bounding box region. **In our images, there are noise from 20px bottom to top.**\n",
        "3. The above function are mapped to all the images using TensorFlow operation."
      ],
      "metadata": {
        "id": "8nBDiBaGu3rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_augmentation(image, label=None, image_size=(512, 512)):\n",
        "    img = random_flip_left_right(image)\n",
        "    img = crop_to_bounding_box(img, 0, 0, 512-20, 512)\n",
        "    img = resize(img, image_size)\n",
        "\n",
        "    if label is None:\n",
        "        return img\n",
        "    else:\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "xrMHr_OgtDld"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "    .map(image_decoding, num_parallel_calls=AUTO).map(data_augmentation, num_parallel_calls=AUTO).shuffle(2048).batch(batch_size)\n",
        ")\n",
        "\n",
        "val_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "    .map(image_decoding, num_parallel_calls=AUTO).batch(batch_size)\n",
        ")\n",
        "\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(test_paths)\n",
        "    .map(image_decoding, num_parallel_calls=AUTO).batch(batch_size)\n",
        ")"
      ],
      "metadata": {
        "id": "DtX0nt5XoVMF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Building\n",
        "\n",
        "1. In this model, I've used DenseNet201 pre-trained model with `imagenet` weights.\n",
        "2. Here, We are trying to train different dataset using `imagenet` weights, this concept is called **transfer learning**.\n",
        "3. Chaning the architecture of the pre-trained model (eg. DenseNet201) is called **fine tuning.**\n",
        "4. Last layer is output layer where activation function is `softmax`.\n",
        "5. I've used Stochastic Gradient Descent (SGB) with learning rate 0.0005"
      ],
      "metadata": {
        "id": "ycF2pjSZu7h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "        DenseNet201(weights = 'imagenet',\n",
        "              include_top = False,\n",
        "              input_shape = (512, 512, 3),\n",
        "              ),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.95),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(100, activation = 'softmax')\n",
        "])\n",
        "\n",
        "# The optimiser is stochastic gradient descent with a learning rate of 0.0005:\n",
        "optimizer = SGD(learning_rate=0.0005)\n",
        "model.compile(optimizer = optimizer,\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEgdwkGQoWJ_",
        "outputId": "71e9ab06-029f-4575-9cf9-37fb5a2f903e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74836368/74836368 [==============================] - 4s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " densenet201 (Functional)    (None, 16, 16, 1920)      18321984  \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 16, 16, 1920)      7680      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 1920)      0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 491520)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               49152100  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67481764 (257.42 MB)\n",
            "Trainable params: 67248868 (256.53 MB)\n",
            "Non-trainable params: 232896 (909.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluation\n",
        "\n",
        "For the evaluation of the model, I've used a function which calculates `Accuracy, Precision, Recall, F1 Score, and Confusion Matrix`."
      ],
      "metadata": {
        "id": "BKpvuWF6u_x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(labels, predictions):\n",
        "  # Calculate accuracy\n",
        "  accuracy = round(accuracy_score(labels, predictions)*100, 3)\n",
        "\n",
        "  # Calculate precision\n",
        "  precision = round(precision_score(labels, predictions, average='macro')*100, 3)\n",
        "\n",
        "  # Calculate recall\n",
        "  recall = round(recall_score(labels, predictions, average='macro')*100, 3)\n",
        "\n",
        "  # Calculate F1-score\n",
        "  f1 = round(f1_score(labels, predictions, average='macro')*100, 3)\n",
        "\n",
        "  # Calculate confusion matrix\n",
        "  cm = confusion_matrix(labels, predictions)\n",
        "\n",
        "  print(f\"Accuracy: {accuracy}\\n\")\n",
        "  print(f\"Precision: {precision}\\n\")\n",
        "  print(f\"Recall: {recall}\\n\")\n",
        "  print(f\"F1-score: {f1}\\n\")\n",
        "  print(\"Confusion Matrix:\\n\")\n",
        "  print(cm)"
      ],
      "metadata": {
        "id": "4L4V_SNbuvuO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Training\n",
        "\n",
        "1. I've utilized a callback called `ReduceLROnPlateau` - It **adjusts learning rate dynamically!** ReduceLROnPlateau reduce models learning rate when specified matrix stops improving.\n",
        "\n",
        "1. I've trained the model with `40` epochs and later the weights of the model are stored in the specified directory with name `aircraft_weights_checkpoints`."
      ],
      "metadata": {
        "id": "X0JYgP7gvfW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 40\n",
        "print('Training...\\n')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "              monitor = 'val_loss',\n",
        "              factor = 0.5,\n",
        "              patience = 5,\n",
        "              min_lr = 0.0001)\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                batch_size = batch_size,\n",
        "                epochs = EPOCHS,\n",
        "                validation_data = val_dataset,\n",
        "                callbacks = [reduce_lr],\n",
        "                verbose = 1)\n",
        "\n",
        "# Save the model\n",
        "model.save(f'{ROOT_FOLDER}/aircraft_weights_checkpoints', save_format='tf')\n",
        "print(\"************* Model saved successfully *************\")"
      ],
      "metadata": {
        "id": "saDUzaaUoXnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4068b406-7428-4a96-e87c-f4c82edbb85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "\n",
            "Epoch 1/40\n",
            "105/105 [==============================] - 629s 3s/step - loss: 10.4559 - accuracy: 0.0195 - val_loss: 3.9107 - val_accuracy: 0.1623 - lr: 5.0000e-04\n",
            "Epoch 2/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 7.2402 - accuracy: 0.1437 - val_loss: 2.7138 - val_accuracy: 0.3315 - lr: 5.0000e-04\n",
            "Epoch 3/40\n",
            "105/105 [==============================] - 87s 774ms/step - loss: 5.1860 - accuracy: 0.2642 - val_loss: 2.4072 - val_accuracy: 0.4173 - lr: 5.0000e-04\n",
            "Epoch 4/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 3.7119 - accuracy: 0.4091 - val_loss: 2.2177 - val_accuracy: 0.4773 - lr: 5.0000e-04\n",
            "Epoch 5/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 3.0577 - accuracy: 0.4880 - val_loss: 2.1943 - val_accuracy: 0.4962 - lr: 5.0000e-04\n",
            "Epoch 6/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 2.4289 - accuracy: 0.5621 - val_loss: 1.7474 - val_accuracy: 0.5590 - lr: 5.0000e-04\n",
            "Epoch 7/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 1.9839 - accuracy: 0.6269 - val_loss: 1.7515 - val_accuracy: 0.5728 - lr: 5.0000e-04\n",
            "Epoch 8/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 1.6326 - accuracy: 0.6818 - val_loss: 1.7037 - val_accuracy: 0.5905 - lr: 5.0000e-04\n",
            "Epoch 9/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 1.4074 - accuracy: 0.7142 - val_loss: 1.6215 - val_accuracy: 0.6007 - lr: 5.0000e-04\n",
            "Epoch 10/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 1.2863 - accuracy: 0.7343 - val_loss: 1.5549 - val_accuracy: 0.6154 - lr: 5.0000e-04\n",
            "Epoch 11/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 1.1113 - accuracy: 0.7588 - val_loss: 1.5275 - val_accuracy: 0.6340 - lr: 5.0000e-04\n",
            "Epoch 12/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 1.0037 - accuracy: 0.7837 - val_loss: 1.5912 - val_accuracy: 0.6274 - lr: 5.0000e-04\n",
            "Epoch 13/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.8815 - accuracy: 0.8053 - val_loss: 1.5631 - val_accuracy: 0.6409 - lr: 5.0000e-04\n",
            "Epoch 14/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.8261 - accuracy: 0.8194 - val_loss: 1.5498 - val_accuracy: 0.6415 - lr: 5.0000e-04\n",
            "Epoch 15/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.7263 - accuracy: 0.8329 - val_loss: 1.4981 - val_accuracy: 0.6580 - lr: 5.0000e-04\n",
            "Epoch 16/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.6271 - accuracy: 0.8509 - val_loss: 1.5449 - val_accuracy: 0.6466 - lr: 5.0000e-04\n",
            "Epoch 17/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.6235 - accuracy: 0.8584 - val_loss: 1.6011 - val_accuracy: 0.6286 - lr: 5.0000e-04\n",
            "Epoch 18/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.5383 - accuracy: 0.8737 - val_loss: 1.5497 - val_accuracy: 0.6571 - lr: 5.0000e-04\n",
            "Epoch 19/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.5401 - accuracy: 0.8755 - val_loss: 1.5050 - val_accuracy: 0.6670 - lr: 5.0000e-04\n",
            "Epoch 20/40\n",
            "105/105 [==============================] - 87s 774ms/step - loss: 0.4786 - accuracy: 0.8881 - val_loss: 1.5130 - val_accuracy: 0.6580 - lr: 5.0000e-04\n",
            "Epoch 21/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.4348 - accuracy: 0.8956 - val_loss: 1.4528 - val_accuracy: 0.6709 - lr: 2.5000e-04\n",
            "Epoch 22/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.3751 - accuracy: 0.9028 - val_loss: 1.4769 - val_accuracy: 0.6730 - lr: 2.5000e-04\n",
            "Epoch 23/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.4196 - accuracy: 0.8947 - val_loss: 1.4570 - val_accuracy: 0.6760 - lr: 2.5000e-04\n",
            "Epoch 24/40\n",
            "105/105 [==============================] - 87s 774ms/step - loss: 0.4156 - accuracy: 0.9079 - val_loss: 1.4781 - val_accuracy: 0.6715 - lr: 2.5000e-04\n",
            "Epoch 25/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.3980 - accuracy: 0.9088 - val_loss: 1.4644 - val_accuracy: 0.6760 - lr: 2.5000e-04\n",
            "Epoch 26/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.3480 - accuracy: 0.9145 - val_loss: 1.4737 - val_accuracy: 0.6769 - lr: 2.5000e-04\n",
            "Epoch 27/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.3220 - accuracy: 0.9235 - val_loss: 1.4526 - val_accuracy: 0.6856 - lr: 1.2500e-04\n",
            "Epoch 28/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.3135 - accuracy: 0.9244 - val_loss: 1.4394 - val_accuracy: 0.6853 - lr: 1.2500e-04\n",
            "Epoch 29/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.3063 - accuracy: 0.9220 - val_loss: 1.4420 - val_accuracy: 0.6865 - lr: 1.2500e-04\n",
            "Epoch 30/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.3140 - accuracy: 0.9217 - val_loss: 1.4285 - val_accuracy: 0.6799 - lr: 1.2500e-04\n",
            "Epoch 31/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.3215 - accuracy: 0.9247 - val_loss: 1.4368 - val_accuracy: 0.6823 - lr: 1.2500e-04\n",
            "Epoch 32/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.3154 - accuracy: 0.9220 - val_loss: 1.4335 - val_accuracy: 0.6847 - lr: 1.2500e-04\n",
            "Epoch 33/40\n",
            "105/105 [==============================] - 87s 774ms/step - loss: 0.2763 - accuracy: 0.9310 - val_loss: 1.4386 - val_accuracy: 0.6901 - lr: 1.2500e-04\n",
            "Epoch 34/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.2408 - accuracy: 0.9358 - val_loss: 1.4344 - val_accuracy: 0.6886 - lr: 1.2500e-04\n",
            "Epoch 35/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.2695 - accuracy: 0.9337 - val_loss: 1.4200 - val_accuracy: 0.6913 - lr: 1.2500e-04\n",
            "Epoch 36/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.2974 - accuracy: 0.9259 - val_loss: 1.4161 - val_accuracy: 0.6862 - lr: 1.2500e-04\n",
            "Epoch 37/40\n",
            "105/105 [==============================] - 87s 774ms/step - loss: 0.3076 - accuracy: 0.9262 - val_loss: 1.4327 - val_accuracy: 0.6850 - lr: 1.2500e-04\n",
            "Epoch 38/40\n",
            "105/105 [==============================] - 87s 773ms/step - loss: 0.2437 - accuracy: 0.9379 - val_loss: 1.4300 - val_accuracy: 0.6835 - lr: 1.2500e-04\n",
            "Epoch 39/40\n",
            "105/105 [==============================] - 87s 774ms/step - loss: 0.2542 - accuracy: 0.9364 - val_loss: 1.4425 - val_accuracy: 0.6823 - lr: 1.2500e-04\n",
            "Epoch 40/40\n",
            "105/105 [==============================] - 87s 774ms/step - loss: 0.2538 - accuracy: 0.9376 - val_loss: 1.4387 - val_accuracy: 0.6853 - lr: 1.2500e-04\n",
            "************* Model saved successfully *************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Model Testing\n",
        "\n",
        "1. In this, firstly the model is loaded from the directory, followed by prediction process taken place. The prediction was performed on the testing dataset.\n",
        "2. The `Accuracy` was more than *70.057%*. The `Precision` was *71.418%*, The `Recall` was *70.058%* and I've got an `F1 Score` of *69.829%*.\n",
        "3. Lastly, the `Confusion Matrix` was calculated for the testing dataset."
      ],
      "metadata": {
        "id": "Fm1V-HGfvrQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(f'{ROOT_FOLDER}/aircraft_weights_checkpoints')\n",
        "pred = model.predict(test_dataset, verbose=1)\n",
        "test_data['Prediction'] = np.argmax(pred, axis=-1)\n",
        "\n",
        "labels = test_data['Labels']\n",
        "predictions = test_data['Prediction']\n",
        "evaluation(labels, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwdVIPcjanmR",
        "outputId": "a6fa48e9-7567-48d0-806a-dcc36e3fddd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105/105 [==============================] - 311s 2s/step\n",
            "Accuracy: 70.057\n",
            "\n",
            "Precision: 71.418\n",
            "\n",
            "Recall: 70.058\n",
            "\n",
            "F1-score: 69.829\n",
            "\n",
            "Confusion Matrix:\n",
            "\n",
            "[[20  0  0 ...  0  0  0]\n",
            " [ 1 26  0 ...  0  1  0]\n",
            " [ 0  0 14 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ... 28  0  0]\n",
            " [ 0  1  0 ...  0 31  0]\n",
            " [ 0  0  0 ...  0  0 31]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Demo"
      ],
      "metadata": {
        "id": "NeR2dlmfJ8V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(f'{ROOT_FOLDER}/aircraft_weights_checkpoints')\n",
        "testing_data = pd.read_csv(f'{ROOT_FOLDER}/classification_aircraft/test.csv')\n",
        "\n",
        "IMG_PATH = f'{ROOT_FOLDER}/classification_aircraft/fgvc-aircraft-2013b/data/images'\n",
        "\n",
        "testing_paths = testing_data.filename.apply(lambda x: os.path.join(IMG_PATH, x))\n",
        "testing_labels = to_categorical(testing_data.Labels)\n",
        "\n",
        "testing_dataset = (tf.data.Dataset.from_tensor_slices(testing_paths)\n",
        "    .map(image_decoding, num_parallel_calls=AUTO).batch(batch_size)\n",
        ")\n",
        "\n",
        "pred = model.predict(testing_dataset, verbose=1)\n",
        "testing_data['Prediction'] = np.argmax(pred, axis=-1)\n",
        "\n",
        "labels = testing_data['Labels']\n",
        "predictions = testing_data['Prediction']\n",
        "\n",
        "# Evaluation\n",
        "evaluation(labels, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8VyS3ksweq5",
        "outputId": "6f88072d-378f-43f3-9c3c-ff292dc1af25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105/105 [==============================] - 14s 138ms/step\n",
            "Accuracy: 70.057\n",
            "\n",
            "Precision: 71.418\n",
            "\n",
            "Recall: 70.058\n",
            "\n",
            "F1-score: 69.829\n",
            "\n",
            "Confusion Matrix:\n",
            "\n",
            "[[20  0  0 ...  0  0  0]\n",
            " [ 1 26  0 ...  0  1  0]\n",
            " [ 0  0 14 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ... 28  0  0]\n",
            " [ 0  1  0 ...  0 31  0]\n",
            " [ 0  0  0 ...  0  0 31]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "en_OEsUU0xO2"
      }
    }
  ]
}